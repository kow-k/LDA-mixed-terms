{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis\n",
    "#!pip install Spacy\n",
    "#!pip install nltk\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update conda --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install Spacy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import required libraries\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "## NumPy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## MatplotLib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Gesim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import hdpmodel as HDPModel\n",
    "\n",
    "## NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "## Spacy\n",
    "import spacy\n",
    "\n",
    "## pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: DeprecationWarning: invalid escape sequence '\\S'\n",
      "<>:16: DeprecationWarning: invalid escape sequence '\\s'\n",
      "<>:15: DeprecationWarning: invalid escape sequence '\\S'\n",
      "<>:16: DeprecationWarning: invalid escape sequence '\\s'\n",
      "/var/folders/s2/lk8hdt6j10j0xyycw1lbjsm40000gn/T/ipykernel_66558/3074727034.py:15: DeprecationWarning: invalid escape sequence '\\S'\n",
      "  data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
      "/var/folders/s2/lk8hdt6j10j0xyycw1lbjsm40000gn/T/ipykernel_66558/3074727034.py:16: DeprecationWarning: invalid escape sequence '\\s'\n",
      "  data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
      "[nltk_data] Downloading package stopwords to /Users/kowk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## loda data\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_sm',disable = ['parser', 'ner'])\n",
    "\n",
    "#importing the Stopwords to use them\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','for'])\n",
    "\n",
    "#downloading the data\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "\n",
    "data = newsgroups_train.data\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "## cleaning the text \n",
    "def tokeniz(sentences):\n",
    "\n",
    "    for sentence in sentences:\n",
    "         yield(gensim.utils.simple_preprocess(str(sentence), deacc = True))\n",
    "\n",
    "processed_data = list(tokeniz(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp_poste', 'host', 'rac_wam', 'university', 'park', 'line', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'call', 'door', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'report', 'keyword', 'acceleration', 'clock', 'upgrade', 'article', 'line', 'nntp_poste', 'host', 'fair', 'number', 'brave', 'soul', 'upgrade', 'clock', 'oscillator', 'share', 'experience', 'poll', 'send', 'brief', 'message', 'detail', 'experience', 'procedure', 'top', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'card', 'adapter', 'heat_sink', 'hour', 'usage', 'day', 'floppy_disk', 'functionality', 'floppy', 'request', 'summarize', 'day', 'add', 'network', 'knowledge', 'base', 'do', 'clock', 'upgrade', 'answer', 'poll', 'thank'], ['question', 'organization', 'purdue_university', 'engineering', 'computer', 'network', 'line', 'folk', 'give', 'ghost', 'weekend', 'start', 'life', 'sooo', 'm', 'market', 'new', 'machine', 'bit', 'intend', 'm', 'look', 'pick', 'powerbook', 'bunch', 'question', 'answer', 'know', 'dirt', 'next', 'round', 'powerbook', 'introduction', 'expect', 'hear', 'suppose', 'make', 'appearence', 'summer', 'hear', 'access', 'macleak', 'wonder', 'info', 'hear', 'rumor', 'price', 'drop', 'powerbook', 'line', 'one', 'duos', 'go', 's', 'impression', 'display', 'swing', 'get', 'disk', 'feel', 'well', 'display', 'yea', 'look', 'great', 'store', 'good', 'solicit', 'opinion', 'people', 'day', 'day', 'worth', 'take', 'disk', 'size', 'money', 'hit', 'get', 'active', 'display', 'realize', 'subjective', 'question', 've', 'play', 'machine', 'computer', 'store', 'breifly', 'figure', 'opinion', 'use', 'machine', 'prove', 'helpful', 'hellcat', 'perform', 'thank', 'bunch', 'advance', 'info', 'email', 'ill', 'post', 'summary', 'news', 'reading', 'time', 'premium', 'final', 'corner', 'purdue', 'electrical_engineere', 'conviction', 'dangerous_enemie', 'truth', 'lie']]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 5), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1)], [(3, 2), (5, 2), (11, 1), (15, 1), (22, 1), (33, 1), (38, 1), (39, 1), (40, 2), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 4), (49, 1), (50, 1), (51, 1), (52, 2), (53, 1), (54, 2), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 2), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 3), (80, 1)], [(5, 2), (12, 2), (13, 1), (15, 2), (16, 2), (18, 1), (27, 1), (33, 1), (36, 1), (41, 1), (54, 1), (63, 1), (77, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1), (86, 1), (87, 2), (88, 2), (89, 1), (90, 1), (91, 1), (92, 1), (93, 2), (94, 3), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 1), (101, 1), (102, 1), (103, 1), (104, 2), (105, 1), (106, 1), (107, 1), (108, 1), (109, 1), (110, 3), (111, 1), (112, 1), (113, 1), (114, 1), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 3), (122, 1), (123, 1), (124, 1), (125, 1), (126, 1), (127, 1), (128, 1), (129, 2), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 3), (137, 1), (138, 1), (139, 1), (140, 1), (141, 1), (142, 3), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 1), (149, 1), (150, 1), (151, 2), (152, 1), (153, 1), (154, 1), (155, 1), (156, 1), (157, 1), (158, 1), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('rac_wam', 1),\n",
       "  ('rest', 1),\n",
       "  ('s', 1),\n",
       "  ('see', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('spec', 1),\n",
       "  ('sport', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('university', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)],\n",
       " [('call', 2),\n",
       "  ('day', 2),\n",
       "  ('host', 1),\n",
       "  ('line', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('thank', 1),\n",
       "  ('acceleration', 1),\n",
       "  ('adapter', 1),\n",
       "  ('add', 2),\n",
       "  ('answer', 1),\n",
       "  ('article', 1),\n",
       "  ('attain', 1),\n",
       "  ('base', 1),\n",
       "  ('brave', 1),\n",
       "  ('brief', 1),\n",
       "  ('card', 1),\n",
       "  ('clock', 4),\n",
       "  ('cpu', 1),\n",
       "  ('detail', 1),\n",
       "  ('do', 1),\n",
       "  ('experience', 2),\n",
       "  ('fair', 1),\n",
       "  ('final', 2),\n",
       "  ('floppy', 1),\n",
       "  ('floppy_disk', 1),\n",
       "  ('functionality', 1),\n",
       "  ('heat_sink', 1),\n",
       "  ('hour', 1),\n",
       "  ('keyword', 1),\n",
       "  ('knowledge', 1),\n",
       "  ('message', 1),\n",
       "  ('network', 1),\n",
       "  ('number', 1),\n",
       "  ('oscillator', 1),\n",
       "  ('poll', 2),\n",
       "  ('procedure', 1),\n",
       "  ('rate', 1),\n",
       "  ('report', 1),\n",
       "  ('request', 1),\n",
       "  ('send', 1),\n",
       "  ('share', 1),\n",
       "  ('si', 1),\n",
       "  ('soul', 1),\n",
       "  ('speed', 2),\n",
       "  ('summarize', 1),\n",
       "  ('summary', 1),\n",
       "  ('top', 1),\n",
       "  ('upgrade', 3),\n",
       "  ('usage', 1)],\n",
       " [('day', 2),\n",
       "  ('info', 2),\n",
       "  ('know', 1),\n",
       "  ('line', 2),\n",
       "  ('look', 2),\n",
       "  ('make', 1),\n",
       "  ('s', 1),\n",
       "  ('thank', 1),\n",
       "  ('wonder', 1),\n",
       "  ('answer', 1),\n",
       "  ('final', 1),\n",
       "  ('network', 1),\n",
       "  ('summary', 1),\n",
       "  ('access', 1),\n",
       "  ('active', 1),\n",
       "  ('advance', 1),\n",
       "  ('appearence', 1),\n",
       "  ('bit', 1),\n",
       "  ('breifly', 1),\n",
       "  ('bunch', 2),\n",
       "  ('computer', 2),\n",
       "  ('conviction', 1),\n",
       "  ('corner', 1),\n",
       "  ('dangerous_enemie', 1),\n",
       "  ('dirt', 1),\n",
       "  ('disk', 2),\n",
       "  ('display', 3),\n",
       "  ('drop', 1),\n",
       "  ('duos', 1),\n",
       "  ('electrical_engineere', 1),\n",
       "  ('email', 1),\n",
       "  ('engineering', 1),\n",
       "  ('expect', 1),\n",
       "  ('feel', 1),\n",
       "  ('figure', 1),\n",
       "  ('folk', 1),\n",
       "  ('get', 2),\n",
       "  ('ghost', 1),\n",
       "  ('give', 1),\n",
       "  ('go', 1),\n",
       "  ('good', 1),\n",
       "  ('great', 1),\n",
       "  ('hear', 3),\n",
       "  ('hellcat', 1),\n",
       "  ('helpful', 1),\n",
       "  ('hit', 1),\n",
       "  ('ill', 1),\n",
       "  ('impression', 1),\n",
       "  ('intend', 1),\n",
       "  ('introduction', 1),\n",
       "  ('lie', 1),\n",
       "  ('life', 1),\n",
       "  ('m', 2),\n",
       "  ('machine', 3),\n",
       "  ('macleak', 1),\n",
       "  ('market', 1),\n",
       "  ('money', 1),\n",
       "  ('new', 1),\n",
       "  ('news', 1),\n",
       "  ('next', 1),\n",
       "  ('one', 1),\n",
       "  ('opinion', 2),\n",
       "  ('organization', 1),\n",
       "  ('people', 1),\n",
       "  ('perform', 1),\n",
       "  ('pick', 1),\n",
       "  ('play', 1),\n",
       "  ('post', 1),\n",
       "  ('powerbook', 3),\n",
       "  ('premium', 1),\n",
       "  ('price', 1),\n",
       "  ('prove', 1),\n",
       "  ('purdue', 1),\n",
       "  ('purdue_university', 1),\n",
       "  ('question', 3),\n",
       "  ('reading', 1),\n",
       "  ('realize', 1),\n",
       "  ('round', 1),\n",
       "  ('rumor', 1),\n",
       "  ('size', 1),\n",
       "  ('solicit', 1),\n",
       "  ('sooo', 1),\n",
       "  ('start', 1),\n",
       "  ('store', 2),\n",
       "  ('subjective', 1),\n",
       "  ('summer', 1),\n",
       "  ('suppose', 1),\n",
       "  ('swing', 1),\n",
       "  ('take', 1),\n",
       "  ('time', 1),\n",
       "  ('truth', 1),\n",
       "  ('use', 1),\n",
       "  ('ve', 1),\n",
       "  ('weekend', 1),\n",
       "  ('well', 1),\n",
       "  ('worth', 1),\n",
       "  ('yea', 1)],\n",
       " [('host', 1),\n",
       "  ('know', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('thing', 1),\n",
       "  ('article', 1),\n",
       "  ('number', 1),\n",
       "  ('get', 2),\n",
       "  ('go', 1),\n",
       "  ('address', 1),\n",
       "  ('amber', 1),\n",
       "  ('chip', 1),\n",
       "  ('command', 1),\n",
       "  ('distribution_world', 1),\n",
       "  ('division', 2),\n",
       "  ('fill', 1),\n",
       "  ('humor', 1),\n",
       "  ('information', 1),\n",
       "  ('low_level', 1),\n",
       "  ('nice', 1),\n",
       "  ('person', 1),\n",
       "  ('phone', 1),\n",
       "  ('point', 1),\n",
       "  ('require', 1),\n",
       "  ('scare', 1),\n",
       "  ('sense', 1),\n",
       "  ('ssd_csd', 1),\n",
       "  ('stuff', 1),\n",
       "  ('system', 2),\n",
       "  ('version_pl', 1),\n",
       "  ('weitek', 1),\n",
       "  ('winter', 1),\n",
       "  ('write', 2)]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Building Bigram & Trigram Models\n",
    "bigram      = gensim.models.Phrases(processed_data, min_count = 5, threshold = 100)\n",
    "trigram     = gensim.models.Phrases(bigram[processed_data], threshold = 100)\n",
    "bigram_mod  = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "##function to filter out stopwords\n",
    "def remove_stopwords(texts):\n",
    "\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "## function to create bigrams\n",
    "def create_bigrams(texts):\n",
    "\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "## function to create trigrams\n",
    "\n",
    "def create_trigrams(texts):\n",
    "\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "## function for lemmatization\n",
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "\n",
    "    texts_op = [ ]\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_op.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\n",
    "    return texts_op\n",
    "\n",
    "## removing stopwords, creating bigrams and lemmatizing the text\n",
    "data_wo_stopwords = remove_stopwords(processed_data)\n",
    "data_bigrams = create_bigrams(data_wo_stopwords)\n",
    "data_lemmatized = lemmatize(data_bigrams, allowed_postags = [ 'NOUN', 'ADJ', 'VERB'])\n",
    "\n",
    "\n",
    "#printing the lemmatized data\n",
    "print(data_lemmatized[:3])\n",
    "\n",
    "#creating a dictionary\n",
    "gensim_dictionary = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "\n",
    "#building a corpus for the topic model\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#printing the corpus we created above.\n",
    "print(gensim_corpus[:3]) \n",
    "\n",
    "#we can print the words with their frequencies.\n",
    "[[(gensim_dictionary[id], freq) for id, freq in cp] for cp in gensim_corpus[:4]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.010*line + 0.008*write + 0.007*get + 0.006*say + 0.006*article + '\n",
      "  '0.005*people + 0.005*know + 0.005*make + 0.005*go + 0.005*organization'),\n",
      " (1,\n",
      "  '0.015*line + 0.010*write + 0.008*organization + 0.007*article + 0.007*get + '\n",
      "  '0.006*know + 0.006*nntp_poste + 0.005*host + 0.005*think + 0.005*go'),\n",
      " (2,\n",
      "  '0.014*line + 0.011*write + 0.008*article + 0.007*organization + 0.007*get + '\n",
      "  '0.006*say + 0.005*know + 0.005*people + 0.005*nntp_poste + 0.005*think'),\n",
      " (3,\n",
      "  '0.760*ax + 0.049*max + 0.001*ei + 0.001*wm + 0.001*pl_pl + 0.001*qax + '\n",
      "  '0.001*bhj_bhj + 0.000*wm_wm + 0.000*giz_giz + 0.000*tm'),\n",
      " (4,\n",
      "  '0.010*line + 0.006*write + 0.006*get + 0.005*article + 0.005*organization + '\n",
      "  '0.004*know + 0.004*nntp_poste + 0.004*host + 0.003*use + 0.003*go'),\n",
      " (5,\n",
      "  '0.010*line + 0.006*get + 0.006*write + 0.005*organization + 0.004*article + '\n",
      "  '0.004*nntp_poste + 0.004*host + 0.004*need + 0.003*go + 0.003*know'),\n",
      " (6,\n",
      "  '0.006*line + 0.004*good + 0.004*write + 0.004*think + 0.004*organization + '\n",
      "  '0.003*say + 0.003*know + 0.003*article + 0.003*system + 0.003*moral'),\n",
      " (7,\n",
      "  '0.006*line + 0.004*write + 0.003*organization + 0.003*get + 0.003*article + '\n",
      "  '0.002*nntp_poste + 0.002*know + 0.002*host + 0.002*people + 0.002*use'),\n",
      " (8,\n",
      "  '0.003*line + 0.003*say + 0.002*get + 0.002*write + 0.002*know + 0.002*way + '\n",
      "  '0.002*organization + 0.002*people + 0.002*see + 0.001*article'),\n",
      " (9,\n",
      "  '0.003*line + 0.002*write + 0.002*article + 0.002*use + 0.002*organization + '\n",
      "  '0.001*host + 0.001*say + 0.001*nntp_poste + 0.001*problem + 0.001*water'),\n",
      " (10,\n",
      "  '0.004*line + 0.002*host + 0.002*write + 0.002*get + 0.002*window + '\n",
      "  '0.002*article + 0.002*organization + 0.002*nntp_poste + 0.001*know + '\n",
      "  '0.001*game'),\n",
      " (11,\n",
      "  '0.003*line + 0.002*write + 0.002*get + 0.001*article + 0.001*know + '\n",
      "  '0.001*organization + 0.001*say + 0.001*nntp_poste + 0.001*people + '\n",
      "  '0.001*thank'),\n",
      " (12,\n",
      "  '0.002*line + 0.001*write + 0.001*get + 0.001*article + 0.001*organization + '\n",
      "  '0.001*invade + 0.001*nntp_poste + 0.001*say + 0.001*know + 0.001*time'),\n",
      " (13,\n",
      "  '0.001*launch + 0.001*mission + 0.001*line + 0.001*probe + 0.001*space + '\n",
      "  '0.001*orbiter + 0.001*cassini + 0.001*study + 0.001*article + 0.001*system'),\n",
      " (14,\n",
      "  '0.007*c + 0.005*_ + 0.003*zd + 0.002*m + 0.002*qs + 0.002*sy + 0.002*ai + '\n",
      "  '0.001*rchz + 0.001*sq + 0.001*x_scx'),\n",
      " (15,\n",
      "  '0.002*year + 0.002*rate + 0.001*car + 0.001*go + 0.001*get + 0.001*line + '\n",
      "  '0.001*insurance + 0.001*m + 0.001*article + 0.001*write'),\n",
      " (16,\n",
      "  '0.001*point + 0.001*find + 0.001*line + 0.001*religion + 0.001*world + '\n",
      "  '0.001*group + 0.001*plane + 0.001*vote + 0.001*people + 0.001*religious'),\n",
      " (17,\n",
      "  '0.003*period + 0.003*power_play + 0.002*second + 0.001*first + '\n",
      "  '0.001*ny_islander + 0.001*scoring + 0.001*unassisted + 0.001*line + '\n",
      "  '0.001*encryption + 0.001*article'),\n",
      " (18,\n",
      "  '0.002*line + 0.002*article + 0.002*write + 0.001*nntp_poste + 0.001*get + '\n",
      "  '0.001*host + 0.001*organization + 0.001*take + 0.001*movie + 0.001*people'),\n",
      " (19,\n",
      "  '0.001*absolute + 0.001*write + 0.001*line + 0.001*article + '\n",
      "  '0.001*organization + 0.001*true + 0.001*money + 0.001*hismanal + '\n",
      "  '0.001*federal + 0.001*land')]\n"
     ]
    }
   ],
   "source": [
    "#creating hdp model\n",
    "hdp_model = HDPModel.HdpModel(corpus = gensim_corpus, id2word = gensim_dictionary)\n",
    "\n",
    "#viewing topics\n",
    "pprint(hdp_model.print_topics())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
