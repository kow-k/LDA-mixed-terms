{
 "cells": [
  {
   "cell_type": "raw",
   "id": "4a988fdd-87bf-4ea8-9fd4-b39dd84a0013",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "LDA で分野混合の用語クラスタリングを実行する\n",
    "開発: 黒田 航 (kow.kuroda@gmail.com)\n",
    "\n",
    "2024/02/17 に料理用語の処理を追加\n",
    "2024/03/01 カタカナを含む語の除外処理を追加\n",
    "2024/03/21 i) FastText のエンコードの追加利用を可能にした, ii) aberrated terms の認識処理を追加\n",
    "2024/03/22 t-SNE の代わりにUMAP を使えるようにした\n",
    "2024/03/24 FastText を使ったエンコードの（ch1gram がterm になっていなかった）バグを修正\n",
    "2024/04/25 DBSCAN clustering をクラスター数を自動で色数の上限の24以下の最大値にする処理を実装\n",
    "2024/04/28 i) クラスターの配属傾向の相関を類似度とし，そのHeatmap を使った可視化を実装, ii) philosophical を追加\n",
    "2024/04/29 LDA に拠るterms のクラスタリングを追加\n",
    "2024/06/28 skippy n-gram の生成を一般的な n に拡張: grams_skippy.py の代わりに gen_ngrams.py が必要\n",
    "2025/08/02 Python 3.9で動作しない事を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e7c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install 'pyLDAvis>3,<3.4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e09714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U plotly\n",
    "#!pip3 install -U seaborn\n",
    "#!pip3 install -U adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9a5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NBConvert のために\n",
    "#!pip install -U nbconvert\n",
    "#!pip install -U pyppeteer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe13212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## imports\n",
    "import sys, os, random, re, glob, copy\n",
    "import pprint as pp\n",
    "#import pandas as pd\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd47d66",
   "metadata": {},
   "source": [
    "実行パラメターの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169c06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc, term の設定\n",
    "doc_type          = 'word'      # 変更不可\n",
    "term_size         = 'character' # 変更不可\n",
    "term_types        = [ '1gram', '2gram', '3gram', '4gram', '5gram',\n",
    "                   'skippy2gram', 'skippy3gram', 'skippy4gram' ]  \n",
    "lda_term_type     = term_types[-1]\n",
    "print(f\"term_type to use: {lda_term_type}\")\n",
    "## doc の最大長と最小長\n",
    "discard_too_long  = True\n",
    "max_doc_length    = 15 # effective if discard_too_long is True\n",
    "discard_too_short = True\n",
    "min_doc_length    = 3 # effective if discard_too_short is True\n",
    "## 行の重複を解消する\n",
    "remove_duplicated_terms = True\n",
    "## Raw term の濾過: unigram から除外する\n",
    "remove_highly_frequent_unigrams = True\n",
    "cutoff_rate             = 0.003 # discard the most frequest terms by <rate>\n",
    "## 特定形態の除外\n",
    "remove_stopwords        = False\n",
    "## カタカナを含む語の除外\n",
    "discard_kana_overloaded = True\n",
    "kana_tolerance          = 0.75\n",
    "## n-gram を包括 (n-gram が (n-1)を真に含む) 型にするかどうか\n",
    "## 包括型にしないと，n が大きくなった時にエンコードされない事例が生じる\n",
    "ngram_is_inclusive = True\n",
    "print(f\"ngram_is_inclusive: {ngram_is_inclusive}\")\n",
    "## doc への境界記号の追加\n",
    "boundary_symbol = \"#\"\n",
    "add_boundaries  = False # Likely to cause errors at adjustText\n",
    "print(f\"boundary_symbol: {boundary_symbol}\")\n",
    "print(f\"add_boundaries: {add_boundaries}\")\n",
    "## skippy n-gram の記号\n",
    "gap_mark = \"…\"\n",
    "print(f\"gap_mark: {gap_mark}\")\n",
    "## skippy n-gram が結びつく最長の gram数: bigram と trigram で共有\n",
    "max_gap_val = round(max_doc_length * 0.75)\n",
    "print(f\"skippy n-gram max_gap: {max_gap_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA の設定\n",
    "## DTM のterm 濾過の設定\n",
    "minfreq = 2 # 最低頻度\n",
    "abuse_threshold = 0.01 # 値が小さい方が濾過力が大きい．0.1 は十分に大きい．\n",
    "## documents のencoding でLDA を使うか\n",
    "use_LDA        = True\n",
    "## topic 数の指定\n",
    "## 1) term_type が複雑になるほど，多くのtopic が高精度で認識される？\n",
    "## 2) 連続n-gram なら，topic 数は用語分類が目的であれば，分野数ぐらいが最適？\n",
    "n_topics       = 30\n",
    "## alpha の決め方\n",
    "lda_uses_fixed_alpha = False # Uses 'auto'\n",
    "lda_alpha_val = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FastText 用の設定\n",
    "## documents のencoding でFastText を使うか: LDA と併用可能\n",
    "use_FastText  = True\n",
    "## FT corpus 構築で使う term_type\n",
    "ft_term_types = [ '1gram', '2gram', '3gram', '4gram', '5gram',\n",
    "                 'skippy2gram', 'skippy3gram', 'skippy4gram' ]\n",
    "ft_term_type  = ft_term_types[2]\n",
    "## FastText のパラメター window の値: skippy n-gram のn に相当\n",
    "ft_window_size = 5 # FastText's default value is 5\n",
    "print(f\"FastText creates encoding vectors based on {ft_window_size}-grams of character {ft_term_type}\")\n",
    "## dims（LDA のn_topics に相当）の指定\n",
    "ft_n_dims_factor = 0.5\n",
    "ft_n_dims = round(n_topics * ft_n_dims_factor)\n",
    "## 設定の確認\n",
    "assert use_LDA or use_FastText\n",
    "if use_FastText:\n",
    "    print(f\"FastText creates encoding vectors of {ft_n_dims} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece4efe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tSNE の設定\n",
    "## topic の tSNE 用の設定\n",
    "top_perplexity_reduct_rate = 0.5\n",
    "print(f\"top_perplexity_rate: {top_perplexity_reduct_rate}\")\n",
    "## doc の tSNE 用の設定\n",
    "doc_perplexity_reduct_rate1 = 1.0 # topic数に依存する場合\n",
    "doc_perplexity_reduct_rate2 = 0.2 # doc数に依存する場合\n",
    "## doc_perplexity を n_topics に基づいて決めるか事例数に基づいて決めるかの選択\n",
    "doc_perplexity_depends_on_n_topics = False\n",
    "if doc_perplexity_depends_on_n_topics:\n",
    "    doc_perplexity_reduc_rate = doc_perplexity_reduct_rate1\n",
    "else:\n",
    "    doc_perplexity_reduct_rate = doc_perplexity_reduct_rate2\n",
    "print(f\"doc_perplexity_reduct_rate: {doc_perplexity_reduct_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66b44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UMAP の設定\n",
    "use_UMAP = True # t-SNE を使った documents の次元圧縮を無効化\n",
    "## n_neighbours, min_dist の最適化は事前に行ってあるが，変更する事は可能\n",
    "umap_n_neighbors = 6\n",
    "umap_min_dist    = 0.2\n",
    "## 距離指標の選択: このデータに関しては，correlation の他の距離指標は効果的でない\n",
    "## 事がわかっている\n",
    "umap_metrics = [    'correlation',  # effective\n",
    "                    'cosine', 'euclidean', # ineffectives\n",
    "                    'canberra', 'braycurtis', 'manhattan', 'minkowski', # poor\n",
    "                    'mahalanobis' # fails\n",
    "                ]\n",
    "#\n",
    "umap_metric = umap_metrics[0]\n",
    "##\n",
    "if use_UMAP:\n",
    "    use_tSNE = False\n",
    "    print(f\"UMAP uses <{umap_metric}> for metric\")\n",
    "else:\n",
    "    use_tSNE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb668250",
   "metadata": {},
   "source": [
    "使用データの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa33279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一般設定\n",
    "verbose           = False\n",
    "save_data         = False\n",
    "save_df_sampled   = False\n",
    "## 分野の設定\n",
    "domain_back_encoding = { 0: 'medical', 1: 'mce',\n",
    "                        2: 'economic', 3: 'juridical', 4: 'publishing', 5: 'cooking',\n",
    "                        6: 'philosophical', 7: 'buddhist',\n",
    "                        8: 'other'\n",
    "                        }\n",
    "domain_encoding = { v: k for k, v in domain_back_encoding.items() }\n",
    "domain_names = domain_encoding.keys()\n",
    "## juridical のデータ拡張をするか\n",
    "add_juridic2 = True\n",
    "## 誤用を取り込むか\n",
    "include_aberrated = True # aberrated medical terms を含めるか\n",
    "## 解析対象分野の選別\n",
    "domain_selection = { 'medical'       : True,\n",
    "                     'mce'           : True,\n",
    "                     'economic'      : False,\n",
    "                     'juridical'     : False,\n",
    "                     'publishing'    : False,\n",
    "                     'cooking'       : True,\n",
    "                     'philosophical' : True,\n",
    "                     'buddhist'      : False\n",
    "                     }\n",
    "## 列名の定義\n",
    "target_vars = [ doc_type ]\n",
    "target_vars.extend(domain_selection.keys())\n",
    "print(f\"target_vars: {target_vars}\")\n",
    "## 分野の選択\n",
    "selected_domains = { k: v for k, v in domain_selection.items() if v == True } # as dictionary\n",
    "print(f\"selected domains: {[ k for k, v in selected_domains.items() if v == True ]}\")\n",
    "## 注目する分野の指定\n",
    "target_domain = 'medical'\n",
    "try:\n",
    "    assert target_domain in selected_domains.keys()\n",
    "except AssertionError:\n",
    "    print(f\"target {target_domain} not in selected domains\")\n",
    "## 分野毎の事例数の均衡化\n",
    "domain_sample_size = 400\n",
    "if len(selected_domains) > 1:\n",
    "    balanced = True\n",
    "else:\n",
    "    balanced = False\n",
    "print(f\"domain balancing: {balanced} with {domain_sample_size} samples from each domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3efda13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## colormap の定義: N.B. Plotly go.Scatter_3D(..) では有効でない\n",
    "used_colors =  [ 'deeppink', 'salmon', 'pink',  'orange', 'green', 'khaki', 'skyblue', 'gray', 'black' ]\n",
    "colormap = { k : used_colors[k] for k, v in domain_back_encoding.items() if v in selected_domains.keys() }\n",
    "if verbose:\n",
    "    print(colormap)\n",
    "for k, v in colormap.items():\n",
    "    print(f\"分野: {k} {domain_back_encoding[k]}\".ljust(20) + \" => \".rjust(-10) + f\"色: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbab5c18",
   "metadata": {},
   "source": [
    "ファイルを読み込んでデータ構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1e7b3d-c005-4967-978a-27cd826e4e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## load data to process\n",
    "from pathlib import Path\n",
    "import pprint as pp\n",
    "wd = Path(\".\")\n",
    "dirs = [ x for x in wd.iterdir() if x.is_dir() and not x.match(r\"plot*\") ]\n",
    "if verbose:\n",
    "    print(f\"The following {len(dirs)} directories are potential targets:\")\n",
    "    pp.pprint(sorted(dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8460a82-a262-4379-8b8f-6aab9f8a262c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## list up files in target directory \n",
    "targetdir = \"terms-source\" # can be changed\n",
    "files = sorted(list(wd.glob(f\"{targetdir}/terms*.xlsx\")))\n",
    "#\n",
    "print(f\"\\n{targetdir} contains {len(files)} files to process\")\n",
    "pp.pprint(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3eff2-1beb-4f34-9f44-b54daba971fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 本来のファイルの処理\n",
    "files_core = [ fn for fn in files if not \"juridical-v2b\" in str(fn) and not \"errors-v\" in str(fn) ]\n",
    "files_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089f53e4-b886-47b3-ba8a-0bad114682ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 必要に応じて次を実行\n",
    "#!pip install -U openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db042ad0-fe13-42bc-ab8f-46e37eea1d0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## ファイルの読み込み\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "raw_dfs = [ ]\n",
    "for file in files_core:\n",
    "    ## 出版用語のsampled = 1 の場合のみ選別\n",
    "    if \"publishing\" in str(file):\n",
    "        d = pd.read_excel(file) # requires openpyxl to be installed\n",
    "        d = d[d['local.id'] != None]\n",
    "        d = d[d['unique'] == 1] # 重複出現する語を除外\n",
    "        d = d[d['sampled'] == 1]\n",
    "        raw_dfs.append(d)\n",
    "    else:\n",
    "        d = pd.read_excel(file)\n",
    "        d = d[d['local.id'] != None]\n",
    "        d = d[d['unique'] == 1] # 重複出現する語を除外\n",
    "        raw_dfs.append(d) # requires openpyxl to be installed\n",
    "##\n",
    "raw_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014466ad-60d7-4dcf-923c-e6ba98eab74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataFrame 構築\n",
    "check = False\n",
    "data = pd.DataFrame(columns = target_vars)\n",
    "dfs = [ ]\n",
    "for dfx in raw_dfs:\n",
    "    if check:\n",
    "        print(dfx)\n",
    "    try:\n",
    "        #data.loc[:,target_vars] = dfx[target_vars] # This fails\n",
    "        data = dfx[target_vars]\n",
    "    except KeyError:\n",
    "        data.loc[:,'aberrated'] = 0\n",
    "    if check:\n",
    "        print(data)\n",
    "    dfs.append(data)\n",
    "## 統合\n",
    "df0 = pd.concat(dfs)\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539611b-02b1-4488-98f2-d082c81ed831",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 追加ファイルの処理\n",
    "remaining_files = [ file for file in files if not file in files_core ]\n",
    "print(f\"Remaining files: {remaining_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb6451",
   "metadata": {},
   "outputs": [],
   "source": [
    "## juridical2 の追加\n",
    "if add_juridic2:\n",
    "    juridic2_file = [ fn for fn in remaining_files if \"-juridical\" in str(fn) ][0]\n",
    "    print(juridic2_file)\n",
    "    raw_juridic2 = pd.read_excel(juridic2_file)\n",
    "    ## juridical の sampled = 1 で unique = 1 の事例のみを選別\n",
    "    raw_juridic2 = raw_juridic2[ (raw_juridic2['sampled'] == 1) & (raw_juridic2['unique'] == 1) ]\n",
    "    raw_juridic2 = raw_juridic2[ target_vars ]\n",
    "    df0 = pd.concat([df0, raw_juridic2])\n",
    "#\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a403e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aberrated medical terms の追加\n",
    "if include_aberrated:\n",
    "    ## 初期設定\n",
    "    df0['aberrated'] = 0\n",
    "    #\n",
    "    med_error_fn = [ fn for fn in remaining_files if \"-errors\" in str(fn) ][0]\n",
    "    raw_errors = pd.read_excel(med_error_fn)\n",
    "    ## med errors の aberrated = 1 の事例のみを選別\n",
    "    raw_errors = raw_errors[ raw_errors['aberrated'] == 1 ]\n",
    "    raw_errors = raw_errors[target_vars]\n",
    "    n_sample_errors = round( 0.3 * len(raw_errors))\n",
    "    med_errors = raw_errors.sample(n_sample_errors)\n",
    "    med_errors['aberrated'] = 1\n",
    "    med_errors['domain_id'] = 0\n",
    "    #\n",
    "    df0 = pd.concat([df0, med_errors])\n",
    "#\n",
    "df0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d756410",
   "metadata": {},
   "source": [
    "データの filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 長過ぎる語と短過ぎる語を除外\n",
    "original_len = len(df0)\n",
    "if discard_too_long:\n",
    "    df0 = df0[ [ True if len(str(x)) <= max_doc_length else False for x in df0[doc_type] ] ]\n",
    "#\n",
    "current_len = len(df0)\n",
    "print(f\"{current_len}: {original_len - current_len} cases are removed because they are longer than {max_doc_length}\")\n",
    "\n",
    "if discard_too_short:\n",
    "    df0 = df0[ [ True if len(str(x)) >= min_doc_length else False for x in df0[doc_type] ] ]\n",
    "#\n",
    "current_len = len(df0)\n",
    "print(f\"{current_len}: {original_len - current_len} cases are removed because they are shorter than {min_doc_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf7d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run the following when need arises\n",
    "#%pip install -U regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b111762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## カタカナ/ひらがなを多く含む語の除外\n",
    "def katakana_count(s: str):\n",
    "    import regex\n",
    "    p = regex.compile(r'\\p{Script=Katakana}')\n",
    "    return len(p.findall(s))\n",
    "\n",
    "def hiragana_count(s: str):\n",
    "    import regex\n",
    "    p = regex.compile(r'\\p{Script=Hiragana}')\n",
    "    return len(p.findall(s))\n",
    "#\n",
    "original_len = len(df0)    \n",
    "if discard_kana_overloaded:\n",
    "    ## katakana\n",
    "    df0 = df0[ [ True if katakana_count(str(x))/len(str(x)) <= kana_tolerance else False for x in df0[doc_type] ] ]\n",
    "    ## hiragana\n",
    "    df0 = df0[ [ True if hiragana_count(str(x))/len(str(x)) <= kana_tolerance else False for x in df0[doc_type] ] ]\n",
    "#\n",
    "current_len = len(df0)\n",
    "print(f\"{current_len}: {original_len - current_len} cases are removed because of too many kana characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d4fa5a-0c05-4d98-9cd4-c11a78415bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 色分けの為の domain_id を定義\n",
    "check = False\n",
    "domain_id_list = [ ]\n",
    "for i, row in df0.iterrows():\n",
    "    ## specific domains\n",
    "    if   row['medical']       == 1: domain_id_list.append(domain_encoding['medical'])\n",
    "    elif row['mce']           == 1: domain_id_list.append(domain_encoding['mce'])\n",
    "    elif row['economic']      == 1: domain_id_list.append(domain_encoding['economic'])\n",
    "    elif row['juridical']     == 1: domain_id_list.append(domain_encoding['juridical'])\n",
    "    elif row['publishing']    == 1: domain_id_list.append(domain_encoding['publishing'])\n",
    "    elif row['cooking']       == 1: domain_id_list.append(domain_encoding['cooking'])\n",
    "    elif row['philosophical'] == 1: domain_id_list.append(domain_encoding['philosophical'])\n",
    "    elif row['buddhist']      == 1:\n",
    "        if 'buddhist' in selected_domains:\n",
    "            domain_id_list.append(domain_encoding['buddhist'])\n",
    "        else:\n",
    "            domain_id_list.append(domain_encoding['philosophical'])\n",
    "    ## all others\n",
    "    else:\n",
    "        domain_id_list.append(domain_encoding['other'])\n",
    "\n",
    "## domain_id 列の追加\n",
    "df0['domain_id'] = domain_id_list\n",
    "\n",
    "## domain 列の追加\n",
    "df0['domain'] = df0['domain_id'].apply(lambda x: domain_back_encoding[x])\n",
    "\n",
    "## domain の事例数の確認\n",
    "df0['domain'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61776c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 重複の削除\n",
    "if remove_duplicated_terms:\n",
    "    df0 = df0.drop_duplicates(subset = [doc_type], keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a39e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 分野の事例数の確認\n",
    "print(f\"selected domains\")\n",
    "#S = [ ]\n",
    "for domain_name in selected_domains.keys():\n",
    "    print(f\"domain_name: {domain_name}\")\n",
    "    S = df0[df0[domain_name] == 1]\n",
    "    S.loc[:,'domain'] = domain_name\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 分野指定の整合性の確認\n",
    "#effective_vars = [ 'medical', 'mce', 'economic', 'juridical', 'publishing', 'cooking' ]\n",
    "#domain_names = id2domaing_mapping.values()\n",
    "effective_vars = [ x for x in domain_names if x != 'other' ]\n",
    "df0['checksum'] = df0[effective_vars].sum(axis = 1, skipna = True)\n",
    "print(f\"count of checksum == 0: {len(df0[df0['checksum'] == 0])}\")\n",
    "print(f\"count of checksum  > 1: {len(df0[df0['checksum'] > 1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84162e1d",
   "metadata": {},
   "source": [
    "Balancing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f8f21-11be-4997-94b1-939683d6322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 混合のための割合を調整:　事例数を揃えるために復元抽出の設定にしているので事例の重複が起きる\n",
    "if balanced:\n",
    "    # Medical\n",
    "    if 'medical' in selected_domains.keys():\n",
    "        med_sampled = df0[ df0['medical'] == 1 ].sample(domain_sample_size, replace = False)\n",
    "    else:\n",
    "        med_sampled = None\n",
    "    # MCE\n",
    "    if 'mce' in selected_domains.keys():\n",
    "        mce_sampled = df0[ df0['mce'] == 1 ].sample(domain_sample_size, replace = False)\n",
    "    else:\n",
    "        mce_sampled = None\n",
    "    # Economical\n",
    "    if 'economic' in selected_domains.keys():\n",
    "        econo_sampled = df0[ df0['economic'] == 1 ]\n",
    "    else:\n",
    "        econo_sampled = None\n",
    "    # Juridical\n",
    "    if 'juridical' in selected_domains.keys():\n",
    "        jurid_sampled = df0[ df0['juridical'] == 1 ].sample(domain_sample_size, replace = True)\n",
    "    else:\n",
    "        jurid_sampled = None\n",
    "    # Publishing\n",
    "    if 'publishing' in selected_domains.keys():\n",
    "        pub_sampled = df0[ df0['publishing'] == 1 ].sample(domain_sample_size, replace = True)\n",
    "    else:\n",
    "        pub_sampled = None\n",
    "    # Cooking\n",
    "    if 'cooking' in selected_domains.keys():\n",
    "        cook_sampled = df0[ df0['cooking'] == 1 ].sample(domain_sample_size, replace = False)\n",
    "    else:\n",
    "        cook_sampled = None\n",
    "    # Philosophical\n",
    "    if 'philosophical' in selected_domains.keys():\n",
    "        philo_sampled = df0[ df0['philosophical'] == 1 ]\n",
    "    else:\n",
    "        philo_sampled = None\n",
    "    # Buddhist\n",
    "    if 'buddhist' in selected_domains.keys():\n",
    "        budd_sampled = df0[ df0['buddhist'] == 1 ]\n",
    "    else:\n",
    "        budd_sampled = None\n",
    "    ## integration\n",
    "    df0 = pd.concat([med_sampled, mce_sampled, econo_sampled, jurid_sampled, pub_sampled,\n",
    "                     cook_sampled, philo_sampled, budd_sampled ])\n",
    "\n",
    "## domain の事例数の確認\n",
    "df0['domain_id'].value_counts(sort = True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ab3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 順序をランダマイズし df0 を df として再定義\n",
    "import sklearn.utils\n",
    "df = sklearn.utils.shuffle(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb4ae1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 設定の確認\n",
    "df[[doc_type, 'domain_id', 'checksum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9a281",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 文字数の分布\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "d = [ len(x) for x in df[doc_type] ]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist(d, bins = 20)\n",
    "ax.set_xlabel('length of doc')\n",
    "ax.set_ylabel('freq')\n",
    "plt.title(f\"Distribution of doc sizes (character counts) after filtering\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e9a270",
   "metadata": {},
   "source": [
    "DTM 構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb3f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 境界記号の追加\n",
    "if add_boundaries:\n",
    "    df[doc_type] = df[doc_type].apply(lambda x: f\"{boundary_symbol}{x}{boundary_symbol}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405178d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 共有される名称の定義\n",
    "docs = df[doc_type]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ea8d86-8719-4ce7-b6a4-d902b0f438d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-gram 生成\n",
    "unigrams = [ ]\n",
    "check = False\n",
    "for x in docs:\n",
    "    if check:\n",
    "        print(x)\n",
    "    u = list(x)\n",
    "    if len(u) > 0:\n",
    "        unigrams.append(u)\n",
    "#\n",
    "random.sample(unigrams, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74dc29-b3e0-47dd-86cb-f4cf8c6b0509",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 名称の設定\n",
    "df['1gram'] = unigrams\n",
    "df['1gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d98c5c-27f9-4486-a0f1-aec0d31b6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-gram の頻度を取得\n",
    "import collections\n",
    "unigrams_all = [ ]\n",
    "[ unigrams_all.extend(L) for L in df['1gram'] ]\n",
    "\n",
    "unigram_freqs = collections.Counter(unigrams_all)\n",
    "unigram_freqs.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678e19b-bfe1-41de-b490-65fb3fccbdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 高頻度 1-gram フィルターの産物を確認\n",
    "highly_frequents = sorted(unigram_freqs, reverse = True)[:round(cutoff_rate * len(df['1gram']))]\n",
    "highly_frequents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be9049",
   "metadata": {},
   "source": [
    "高頻度文字 1-grams を df['1gram'] から除外"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47290bc0-6755-4028-9255-15ae2a68990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 高頻度 1-gram の除外\n",
    "if remove_highly_frequent_unigrams:\n",
    "    df['1gram'].apply(lambda x:\n",
    "                     [ u for u in df['1gram'] if not u in highly_frequents ] )\n",
    "    print(f\"{len(highly_frequents)} unigrams were removed due to high frequency\")\n",
    "else:\n",
    "    print(f\"{len(highly_frequents)} unigrams were not removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ecc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2-gram 生成\n",
    "import gen_ngrams\n",
    "bigrams = [ gen_ngrams.gen_ngrams(x, n = 2, sep = \"\", check = False) for x in df['1gram'] ] \n",
    "## 包括的 2gramの生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, b in enumerate(bigrams):\n",
    "        b.extend(unigrams[i])\n",
    "## 変数の追加\n",
    "df['2gram'] = bigrams\n",
    "df['2gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b1d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3-gram 生成\n",
    "import gen_ngrams\n",
    "trigrams = [ gen_ngrams.gen_ngrams(x, n = 3, sep = \"\", check = False) for x in df['1gram'] ] \n",
    "## 包括的 3gramの生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, t in enumerate(trigrams):\n",
    "        t.extend(bigrams[i])\n",
    "## 変数の追加\n",
    "df['3gram'] = trigrams\n",
    "df['3gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44249697",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4-gram 生成\n",
    "import gen_ngrams\n",
    "quadrigrams = [ gen_ngrams.gen_ngrams(x, n = 4, sep = \"\", check = False) for x in df['1gram'] ]\n",
    "## 包括的 3gramの生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, q in enumerate(quadrigrams):\n",
    "        q.extend(trigrams[i])\n",
    "## 変数の追加\n",
    "df['4gram'] = quadrigrams\n",
    "df['4gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52570bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 2-grams の生成\n",
    "#import ngrams_skippy\n",
    "#skippy2grams = [ ngrams_skippy.gen_skippy2grams(x, missing_mark = gap_mark,\n",
    "#                                                   max_distance = max_gap_val,\n",
    "#                                                   minimize = False, check = False)\n",
    "#                                                for x in df['1gram'] ]\n",
    "## The old code above was replaced by the following generalized one.\n",
    "import gen_ngrams\n",
    "skippy2grams = [ gen_ngrams.gen_skippy_ngrams(x, 2, missing_mark = gap_mark,\n",
    "                                                   max_distance = max_gap_val,\n",
    "                                                   sep = \"\",\n",
    "                                                   #minimize = False,\n",
    "                                                   check = False) for x in df['1gram'] ]\n",
    "## 包括的 skippy 2-grams の生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, b2 in enumerate(skippy2grams):\n",
    "        b2.extend(unigrams[i])\n",
    "## 変数の追加\n",
    "df['skippy2gram'] = skippy2grams\n",
    "df['skippy2gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 3-grams の生成\n",
    "#import ngrams_skippy\n",
    "#skippy3grams = [ ngrams_skippy.gen_skippy3grams(x, missing_mark = gap_mark,\n",
    "#                                                max_distance = max_gap_val,\n",
    "#                                                minimize = False, check = False)\n",
    "#                for x in df[doc_type] ]\n",
    "## The old code above was replaced by the better one.\n",
    "import gen_ngrams\n",
    "skippy3grams = [ gen_ngrams.gen_skippy_ngrams(x, 3, sep = \"\",\n",
    "                                               missing_mark = gap_mark,\n",
    "                                                max_distance = max_gap_val,\n",
    "                                                #minimize = False, \n",
    "                                                check = False) for x in df[doc_type] ]\n",
    "## 包括的 skippy 3-grams の生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, t2 in enumerate(skippy3grams):\n",
    "        t2.extend(skippy2grams[i])\n",
    "## 変数の追加\n",
    "df['skippy3gram'] = skippy3grams\n",
    "df['skippy3gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53275e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## skippy 4-grams の生成\n",
    "check = True\n",
    "#import ngrams_skippy\n",
    "#skippy4grams = [ ngrams_skippy.gen_skippy4grams(x, missing_mark = gap_mark,\n",
    "#                                                    max_distance = max_gap_val,\n",
    "#                                                    minimize = False, check = False)\n",
    "#                                                    for x in df[doc_type] ]\n",
    "import gen_ngrams\n",
    "skippy4grams = [ gen_ngrams.gen_skippy_ngrams(x, 4, sep = \"\",\n",
    "                                              missing_mark = gap_mark,\n",
    "                                              max_distance = max_gap_val,\n",
    "                                              check = False) for x in df[doc_type] ]\n",
    "## 包括的 skippy 4-grams の生成\n",
    "if ngram_is_inclusive:\n",
    "    for i, q2 in enumerate(skippy4grams):\n",
    "        q2.extend(skippy3grams[i])\n",
    "## 変数の追加\n",
    "df['skippy4gram'] = skippy4grams\n",
    "df['skippy4gram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15730136",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 使ったデータの保存\n",
    "if save_data:\n",
    "    import datetime as dt\n",
    "    ct = dt.datetime.now()\n",
    "    ## Pandas で .csv として\n",
    "    output_fn1 = f\"saves/mixed-terms-filtered-{ct.date()}-{str(ct.time())[:2]}\" + \".csv\"\n",
    "    print(f\"saving data to {output_fn1}\")\n",
    "    import pandas as pd\n",
    "    df.to_csv(output_fn1)\n",
    "    ## pickle.dump(..)で\n",
    "    output_fn2 = f\"saves/mixed-terms-filtered-{ct.date()}-{str(ct.time())[:2]}\" + \".p\"\n",
    "    import pickle\n",
    "    print(f\"saving data to {output_fn2}\")\n",
    "    with open(output_fn2, \"wb\") as f:\n",
    "        pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe87d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 解析対象の確認\n",
    "## aberrated == 1 は訓練事例に含めない\n",
    "bots = df[df['aberrated'] == 0][lda_term_type]\n",
    "bots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbfde6d-13a1-49a0-b032-55a73a53fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## stopwords を除外\n",
    "stopwords = [ '腫瘍', '性', '炎', '血', '腫', '瘍', 'の' ]\n",
    "if remove_stopwords:\n",
    "    bows = [ [ x for x in bot if x not in stopwords ] for bot in bots ]\n",
    "if verbose:\n",
    "    random.sample(list(bots), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3e2422-5e03-43c5-b8db-5a6f4fdb922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA 構築の基になる dictionary = document-term matrix (dtm) を構築\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "diction = Dictionary(bots)\n",
    "## 結果の確認\n",
    "print(diction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698d5a7-efcf-4f6b-8271-c244509881bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 構造の確認\n",
    "for k in random.sample(diction.keys(), 20):\n",
    "    print(f\"index: {k} ; term: {diction[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706ba9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## diction の検査\n",
    "import pandas as pd\n",
    "from operator import attrgetter\n",
    "from collections import namedtuple\n",
    "freq_record = namedtuple('record', ['term', 'freq'])\n",
    "term_freqs = [ freq_record(*x) for x in diction.most_common() ] ## most_common allows access to frequencies\n",
    "terms_freqs = sorted(term_freqs, key = attrgetter('freq'), reverse = True)\n",
    "## term の頻度順位で上位30件\n",
    "term_freqs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac376eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 頻度1 のterm の個数\n",
    "hapaxes = [ record for record in term_freqs if record.freq == 1]\n",
    "hapax_counts = {len(hapaxes)}\n",
    "## 頻度1 の term のサンプルn件\n",
    "hapax_sample_n = 20\n",
    "print(f\"{hapax_sample_n} samples\")\n",
    "pp.pprint(random.sample(hapaxes, hapax_sample_n))\n",
    "## 個数\n",
    "print(f\"number of hapaxes: {hapax_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8cfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## diction の濾過\n",
    "import copy\n",
    "diction_copy = copy.deepcopy(diction) # 予備の生成\n",
    "## filter適用: 実は諸刃の刃で，token数が少ない時には適用しない方が良い\n",
    "apply_filter = True\n",
    "if apply_filter:\n",
    "    diction_copy.filter_extremes(no_below = minfreq, no_above = abuse_threshold)\n",
    "print(diction_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e869e-7868-4ad5-9c76-50f17f239c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## データを選択\n",
    "diction = diction_copy # 名前を元に戻す\n",
    "## gensim の用 corpusの構築\n",
    "corpus = [ diction.doc2bow(bot) for bot in bots ]\n",
    "corpus_sample_n = 1\n",
    "print(f\"{corpus_sample_n} samples\")\n",
    "pp.pprint(random.sample(corpus, corpus_sample_n))\n",
    "print(f\"Number of documents: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450a987",
   "metadata": {},
   "source": [
    "LDA を実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532177c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA モデルの構築\n",
    "from gensim.models import LdaModel\n",
    "if lda_uses_fixed_alpha:\n",
    "    doc_lda = LdaModel(corpus, id2word = diction, num_topics = n_topics, alpha = lda_alpha_val)\n",
    "else:\n",
    "    doc_lda = LdaModel(corpus, id2word = diction, num_topics = n_topics, alpha = 'auto')\n",
    "print(doc_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef942917-48c0-4c66-8e29-e37380d57595",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## 結果の検査\n",
    "sample_n = 3\n",
    "for doc in random.sample(list(df[doc_type]), sample_n):\n",
    "    print(doc)\n",
    "    p_dist = doc_lda.get_document_topics(diction.doc2bow([f\"{doc}\"]), minimum_probability = 0)\n",
    "    print(p_dist)\n",
    "    print(f\"sums up to: {sum(p_dist[-1]):0.3f} over {len(p_dist)} topics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5e9cb",
   "metadata": {},
   "source": [
    "LDAvis の実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e987470",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## pyLDAvis を使った結果 D_lda の可視化: 階層クラスタリングより詳しい\n",
    "import pyLDAvis\n",
    "installed_version = pyLDAvis.__version__\n",
    "print(f\"pyLDAvis installed version: {installed_version}\")\n",
    "#\n",
    "if float(installed_version[:3]) > 3.1:\n",
    "    import pyLDAvis.gensim_models as gensimvis\n",
    "else:\n",
    "    import pyLDAvis.gensim as gensimvis\n",
    "#\n",
    "pyLDAvis.enable_notebook()\n",
    "#\n",
    "lda_used     = doc_lda\n",
    "corpus_used  = corpus\n",
    "diction_used = diction\n",
    "## 実行パラメター: MMDS かtSNEを選ぶと \"JSON object is too complex\" error を回避できる \n",
    "LDAvis_use_MMDS = False\n",
    "LDAvis_use_tsne = False\n",
    "if LDAvis_use_MMDS:\n",
    "    vis = gensimvis.prepare(lda_used, corpus_used, diction_used, mds = 'mmds', n_jobs = 1, sort_topics = False)\n",
    "elif LDAvis_use_tsne:\n",
    "    vis = gensimvis.prepare(lda_used, corpus_used, diction_used, mds = 'tsne', n_jobs = 1, sort_topics = False)\n",
    "else:\n",
    "    vis = gensimvis.prepare(lda_used, corpus_used, diction_used, n_jobs = 1, sort_topics = False)\n",
    "#\n",
    "pyLDAvis.display(vis)\n",
    "## topic を表わす円の重なりが多いならn_topics が多過ぎる可能性がある．\n",
    "## ただし2Dで重なっていても，3Dなら重なっていない可能性もある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3df34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## lda のtopic ごとに，関連度の高い term を表示\n",
    "import pandas as pd\n",
    "n_terms = 30 # topic ごとに表示する term 数の指定\n",
    "topic_dfs = [ ]\n",
    "for topic in range(n_topics):\n",
    "    terms = [ ]\n",
    "    for i, prob in doc_lda.get_topic_terms(topic, topn = n_terms):\n",
    "        terms.append(diction.id2token[ int(i) ])\n",
    "    #\n",
    "    topic_dfs.append(pd.DataFrame([terms], index = [ f'topic {topic+1}' ]))\n",
    "#\n",
    "topic_term_df = pd.concat(topic_dfs)\n",
    "## Table で表示\n",
    "topic_term_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01953276",
   "metadata": {},
   "source": [
    "Topics のクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e5644-5510-42f5-acf9-295253c3d901",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA がD に対して生成した topics の弁別性を確認\n",
    "## 得られたtopics を確認\n",
    "topic_dist = doc_lda.get_topics()\n",
    "if check:\n",
    "    random.sample(list(topic_dist), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0063e1b-c379-419c-b66b-012beb18c413",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 検査 1: topic ごとに分布の和を取る\n",
    "topic_dist.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67262e-e8c3-4391-a82b-987e8864c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 検査 2: 総和を求める: n_topics に (ほぼ) 等しいなら正常\n",
    "round(topic_dist.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75428e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D_topic_dist を使った topic の階層クラスタリング\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "## 作図範囲の指定\n",
    "plt.figure(figsize = (5, 6))\n",
    "## 距離行列の生成\n",
    "topic_linkage = linkage(topic_dist, method = 'ward', metric = 'euclidean')\n",
    "dendrogram(topic_linkage, orientation = 'left')\n",
    "## 作図\n",
    "plt.xlim(0.5, 0) # 異なる分析に比較を楽にするために，x軸の大きさを固定\n",
    "plt.title(f\"Hirarchical clustering of {n_topics} topics [0-based]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf4e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tSNE を使った topic のクラスタリング\n",
    "import sklearn.manifold\n",
    "## 必要に応じて reload\n",
    "need_reloading = False\n",
    "if (need_reloading):\n",
    "    import importlib\n",
    "    importlib.reload(sklearn)\n",
    "## tSNE のパラメターを設定\n",
    "## n_components は射影先の空間の次元: n_components = 3 なら3次元空間に射影\n",
    "## perplexity は結合の強さを表わす指数で，値に拠って結果が代わるので，色々な値を試すと良い\n",
    "relative = True\n",
    "if relative:\n",
    "    #top_perplexity_reduct_rate = 0.5 # 定義は先頭に移動\n",
    "    top_perplexity_val = round(n_topics * top_perplexity_reduct_rate)\n",
    "else:\n",
    "    top_perplexity_val = 5 # 大き過ぎると良くない\n",
    "print(f\"top_perplexity_val: {top_perplexity_val}\")\n",
    "## 3D version\n",
    "top_tSNE_3d = sklearn.manifold.TSNE(n_components = 3, random_state = 0,\n",
    "                                perplexity = top_perplexity_val, n_iter = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4ccf66-7077-4a35-8dd1-1bd0c23f2131",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic_dist データに適用 topic_dist のグループ化 (3D)\n",
    "topic_tsne_3d = top_tSNE_3d.fit_transform(topic_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6eeff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 必要に応じて Plotly を導入\n",
    "#!pip3 install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e28cc3-34af-4760-bd55-61fa4ea0f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## Plotlyを使って tSNE の結果の可視化 (3D)\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "## 変数の指定\n",
    "d1, d2, d3 = topic_tsne_3d[:,0], topic_tsne_3d[:,1], topic_tsne_3d[:,2]\n",
    "fig = go.Figure(data = [go.Scatter3d(x = d1, y = d2, z = d3,\n",
    "                                     mode = 'markers', marker = dict(size = 7)) ])\n",
    "## 3D 散布図にラベルを追加する処理は未実装\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_val = f\"tSNE 3D (ppl: {top_perplexity_val}) of topics from LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_val = f\"tSNE 3D (ppl: {top_perplexity_val}) of dims under FastText ({n_topics} dims)\"\n",
    "else:\n",
    "    title_val = f\"tSNE 3D (ppl: {top_perplexity_val}) of topics from LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "#\n",
    "fig.update_layout(title = dict(text = title_val, font_size = 13),\n",
    "                  autosize = False, width = 600, height = 600,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e7162-4971-4a3b-a3f3-b51f13f9d493",
   "metadata": {},
   "source": [
    "Docs のエンコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1452fbcc-a338-4eb6-a434-ab1c306de037",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA モデルを使ったエンコード\n",
    "check      = False\n",
    "lda_doc_encoding = [ ]\n",
    "print(f\"get {lda_term_type}-based LDA encodings for documents\")\n",
    "for i, row in df.iterrows():\n",
    "    if check:\n",
    "        print(f\"row: {row}\")\n",
    "    doc = row[doc_type]\n",
    "    bot = row[lda_term_type]\n",
    "    ## get_document_topics(..) では　minimu_probability = 0 としないと\n",
    "    ## 値が十分に大きな topics に関してだけ値が取れる\n",
    "    enc_temp = doc_lda.get_document_topics(diction.doc2bow(bot), minimum_probability = 0)\n",
    "    if check:\n",
    "        print(f\"enc_temp: {enc_temp}\")\n",
    "    lda_doc_encoding.append([ x[-1] for x in enc_temp])\n",
    "#\n",
    "len(lda_doc_encoding)\n",
    "if verbose:\n",
    "    random.sample(lda_doc_encoding, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a10cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FastText を使ったエンコード\n",
    "from gensim.models import FastText\n",
    "## build a model: for better comparison, vector_size should be equal to n_topics\n",
    "print(f\"get FastText encodings for documents\")\n",
    "#ft_corpus = df[doc_type]\n",
    "## emulate sentences by concatenating character 1-grams by whitespaces\n",
    "ft_corpus = [ \" \".join(x) for x in df[ft_term_type] ]\n",
    "ft_model = FastText(ft_corpus,\n",
    "                    vector_size = ft_n_dims,\n",
    "                    window = ft_window_size,\n",
    "                    min_count = 1, sg = 1)\n",
    "## ft_model は dict\n",
    "ft_doc_encoding = [ ft_model.wv[doc] for doc in ft_corpus ]\n",
    "#\n",
    "len(ft_doc_encoding)\n",
    "if check:\n",
    "    random.sample(ft_doc_encoding, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89ed630",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc_encoding の選択: LDA か FastText か両方の組み合わせか\n",
    "check = False\n",
    "if use_LDA:\n",
    "    if use_FastText:\n",
    "        #doc_encoding = [ x + y for x, y in zip(lda_encoding, ft_encoding) ]\n",
    "        doc_encoding = [ np.reshape(np.concatenate([x, y], dtype = object) , -1) for x, y in\n",
    "                        zip(lda_doc_encoding, ft_doc_encoding) ]\n",
    "        encoding_method = \"LDA x FastText\"\n",
    "    else:  \n",
    "        doc_encoding = lda_doc_encoding\n",
    "        encoding_method = \"LDA\"\n",
    "else:\n",
    "    doc_encoding = ft_doc_encoding\n",
    "    encoding_method = \"FastText\"\n",
    "len(doc_encoding)\n",
    "## sample の確認\n",
    "if check:\n",
    "    print(random.sample(doc_encoding, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f5976c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## df に enc の列を追加\n",
    "df['enc'] = doc_encoding\n",
    "if verbose:\n",
    "    df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23296f56-c71a-46d8-9fb6-c351ca162661",
   "metadata": {},
   "outputs": [],
   "source": [
    "## エンコーディングのstd の分布を見る\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "std_dist = [ np.std(x) for x in df['enc'] ]\n",
    "plt.hist(std_dist, bins = 20)\n",
    "plt.title(f\"Distribution of std values among encodings\")\n",
    "plt.show()\n",
    "## 0 周辺に事例があるなら，エンコーディング効率が悪い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44386545-b8d6-4e41-b4d0-3d5d090f1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 一様分布の事例を除外\n",
    "import numpy as np\n",
    "\n",
    "check = False\n",
    "print(f\"{len(df)} instances before filtering\")\n",
    "enc = df['enc']\n",
    "\n",
    "max_std = max([ np.std(x) for x in enc])\n",
    "if check:\n",
    "    print(f\"std max: {max_std}\")\n",
    "\n",
    "min_std = min([ np.std(x) for x in enc])\n",
    "if check:\n",
    "    print(f\"std min: {min_std}\")\n",
    "\n",
    "first_min_std = list(sorted(set([ np.std(x) for x in enc])))[-0]\n",
    "print(f\"std 1st min: {first_min_std}\")\n",
    "\n",
    "second_min_std = list(sorted(set([ np.std(x) for x in enc])))[-1]\n",
    "print(f\"std 2nd min: {second_min_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb41541-9309-40ee-9a3d-1ec361073255",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 閾値は2番目に小さい値より小さく最小値よりは大きな値であるべき\n",
    "import numpy as np\n",
    "\n",
    "std_threshold = second_min_std / 9 # 穏健な値を得るために9で割った\n",
    "print(f\"std_threshold: {std_threshold}\")\n",
    "\n",
    "## Rっぽい次のコードは通らない\n",
    "#df_filtered = df[ df['encoding'] > std_threshold ]\n",
    "## 通るのは次のコード: Creating a list of True/False and apply it to DataFrame \n",
    "classified = [ False if np.std(x) < std_threshold else True for x in df['enc'] ]\n",
    "df_filtered = df[ classified ]\n",
    "#\n",
    "print(f\"filtering leaves {len(df_filtered)} instances ({len(df) - len(df_filtered)} instances removed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96b8ea-3f59-40fc-8ca6-eabbf6295477",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 弱いサンプリング\n",
    "secondary_sampling = False\n",
    "if secondary_sampling:\n",
    "    df = df.sample(round(len(df) * 0.5))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fa774-d402-431f-9e2e-e8e44cf0ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['domain_id'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1ebde",
   "metadata": {},
   "source": [
    "Dim Reduct (tSNE or UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4ca231-a5d7-42f9-aed8-ce876d4cb3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tSNE を使った documents のクラスタリング\n",
    "if use_tSNE:\n",
    "    import sklearn.manifold\n",
    "    ## reload on necessity\n",
    "    need_reloading = False\n",
    "    if (need_reloading):\n",
    "        import importlib\n",
    "        importlib.reload(sklearn)\n",
    "    ## n_components は射影先の空間の次元: n_components = 3 なら3次元空間に射影\n",
    "    relative = True\n",
    "    if relative:\n",
    "        doc_perplexity_val = round(len(df) * doc_perplexity_reduct_rate)\n",
    "    else:\n",
    "        doc_perplexity_val = 300 # 大き過ぎると良くない\n",
    "    print(f\"doc_perplexity_val: {doc_perplexity_val}\")\n",
    "    ## 3D version\n",
    "    doc_tSNE_3d = sklearn.manifold.TSNE(n_components = 3, random_state = 0,\n",
    "                                    perplexity = doc_perplexity_val, n_iter = 1000)\n",
    "    ## データに適用\n",
    "    E = np.array(list(df['enc']))\n",
    "    doc_tSNE_3d = doc_tSNE_3d.fit_transform(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb979e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UMAP を使った documents のグループ化\n",
    "import numpy as np\n",
    "if use_UMAP:\n",
    "    #import umap # causes a problem\n",
    "    import umap.umap_ as umap\n",
    "    ## UMAP の生成\n",
    "    UMAP_3d = umap.UMAP(n_components = 3, random_state = 1, n_jobs = 1,\n",
    "                        metric = umap_metric,\n",
    "                        n_neighbors = umap_n_neighbors, min_dist = umap_min_dist)\n",
    "\n",
    "    ## データに適用\n",
    "    doc_UMAP_3d = UMAP_3d.fit_transform(np.array(list(df['enc'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e7dbb2",
   "metadata": {},
   "source": [
    "Dim Reduct の結果の可視化 (Ploty を使って)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa728d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 使用する embedding の選択\n",
    "if use_UMAP:\n",
    "    Fit_3d = doc_UMAP_3d\n",
    "else:\n",
    "    Fit_3d = doc_tSNE_3d\n",
    "## plot_df の定義\n",
    "import pandas as pd\n",
    "encoded_df = pd.DataFrame(\n",
    "    zip(Fit_3d[:,0], Fit_3d[:,1], Fit_3d[:,2], df['domain_id'], df['domain'], df['aberrated'], df[doc_type]),\n",
    "    columns = ['D1', 'D2', 'D3', 'domain_id', 'domain', 'aberrated', doc_type])\n",
    "#\n",
    "encoded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35513cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d5e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## Plotlyを使って tSNE の結果の可視化 (3D)\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "#\n",
    "domain_ids = set(encoded_df['domain_id'])\n",
    "reverse_domain_list = False\n",
    "if reverse_domain_list:\n",
    "    domain_ids = sorted(domain_list, reverse = True)\n",
    "## layered/traced drawing\n",
    "fig = go.Figure()\n",
    "for domain_id in domain_ids:\n",
    "    # set marker size of the target domain\n",
    "    if domain_id == [ i for k, v in domain_encoding.items() if k == target_domain ][0]:\n",
    "        size_val = 5\n",
    "    else:\n",
    "        size_val = 3\n",
    "    part = encoded_df[ encoded_df['domain_id'] == domain_id ]\n",
    "    color = colormap[domain_id]\n",
    "    print(f\"color: {color}\")\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x = part['D1'], y = part['D2'], z = part['D3'],\n",
    "            name = domain_back_encoding[domain_id],\n",
    "            mode = 'markers',\n",
    "            marker = dict(size = size_val, opacity = 0.8),\n",
    "            #marker_colorscale = list(colormap.values()),\n",
    "            marker_color = color,\n",
    "            showlegend = True\n",
    "        )\n",
    "    )\n",
    "\n",
    "## 3D 散布図にラベルを追加する処理は未実装\n",
    "df_size = len(df)\n",
    "if use_UMAP:\n",
    "    title_header = f\"UMAP 3D (metric: {umap_metric}; {umap_n_neighbors} neighbors; min dist: {umap_min_dist}) of {df_size} encodings via\\n\"\n",
    "else:\n",
    "    title_header = f\"t-SNE 3D (ppl: {doc_perplexity_val}) of {df_size} encodings of via\\n\"\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_body = f\"LDA ({n_topics} topics; term:0 {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_body = f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "else:\n",
    "    title_body = f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "#\n",
    "title_val = title_header + title_body\n",
    "fig.update_layout(title = dict(text = title_val, font_size = 12), autosize = False, width = 700, height = 700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65452d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## tSNE の結果の可視化\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "## 日本語表示のための設定\n",
    "plt.rcParams[\"font.family\"] = \"Hiragino sans\" # Windows/Linux の場合は別のフォントを指定\n",
    "## 文字を表示する事例のサンプリング\n",
    "relative = True\n",
    "lab_sampling_rate = 0.02 # サンプリング率の指定\n",
    "if relative:\n",
    "    lab_sample_n = round(lab_sampling_rate * len(df))\n",
    "else:\n",
    "    lab_sample_n = 30 # 絶対数の指定\n",
    "print(f\"lab_sample_n: {lab_sample_n}\")\n",
    "## labels の生成\n",
    "label_size = max_doc_length\n",
    "sampled_keys = [ x[:label_size] for x in random.sample(list(df[doc_type]), lab_sample_n) ]\n",
    "##\n",
    "for i in range(3):\n",
    "    roll = np.roll([0,1,2], -i)\n",
    "    asp1, asp2 = roll[0], roll[1]\n",
    "    X, Y = Fit_3d[:, asp1], Fit_3d[:, asp2]\n",
    "    ## \n",
    "    plt.figure(figsize = (6, 6))\n",
    "    plt.xlim(X.min(), X.max() + 1)\n",
    "    plt.ylim(Y.min(), Y.max() + 1)\n",
    "    ## 分野ごとの色分け\n",
    "    cmap = list(map(lambda i: colormap[i], df['domain_id'])) # colormap は最初に定義してある\n",
    "    scatter = plt.scatter(X, Y, s = 30, c = cmap, edgecolors = 'w')\n",
    "    ##\n",
    "    texts = [ ]\n",
    "    for x, y, s in zip(X, Y, sampled_keys):\n",
    "        texts.append(plt.text(x, y, s, size = 7, color = 'blue'))\n",
    "    ## label に repel を追加: adjustText package の導入が必要\n",
    "    get_Float_object_is_not_subsriptable_Error = False\n",
    "    if get_Float_object_is_not_subsriptable_Error:\n",
    "        pass\n",
    "    else:\n",
    "        try:\n",
    "            adjust_text(texts,\n",
    "                    expand_points = (1, 1), expand_text = (1, 1),\n",
    "                    force_points = 0.2,\n",
    "                    force_text = 0.2,\n",
    "                    arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "        except TypeError:\n",
    "            pass\n",
    "    ## title piece 1\n",
    "    if use_UMAP:\n",
    "        embedding_method = f\"UMAP (metric: {umap_metric}; n_neighbors: {umap_n_neighbors}; min_dist: {umap_min_dist})\"\n",
    "    else:\n",
    "        embedding_method = f\"t-SNE (ppl: {doc_perplexity_val})\"\n",
    "    ## title pice 2\n",
    "    used_domains = f\"<{', '.join(selected_domains.keys())}>\"\n",
    "    ## title_val\n",
    "    title_header = f\"2D view (D{asp1+1}, D{asp2+1}) of {embedding_method} for {len(df)} docs\\nin {used_domains} via\\n\"\n",
    "    if use_FastText:\n",
    "        if use_LDA:\n",
    "            title_val = title_header + f\"LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "        else:\n",
    "            title_val = title_header + f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_val = title_header + f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "    #\n",
    "    plt.title(title_val)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b1ec69",
   "metadata": {},
   "source": [
    "DBSCAN で docs をクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DBSCAN でクラスタリング\n",
    "from sklearn.cluster import DBSCAN\n",
    "## source の指定\n",
    "dbscan_source = Fit_3d\n",
    "## eps, min_samples は事例ごとに調節が必要\n",
    "min_samples_val = 2\n",
    "## looking for optimal eps val compatible with color palette\n",
    "max_n_clusters = 24 # This depends on the differetiation in color palette used\n",
    "dbscan_clustered = None\n",
    "cluster_ids = None\n",
    "print(f\"Looking for the optimal eps val...\")\n",
    "max_val = 5 # needs to be effectively large\n",
    "eps_vals = np.arange(max_val, 0.005, -0.05)\n",
    "for eps_val in eps_vals:\n",
    "    if verbose:\n",
    "        print(f\"testing eps = {eps_val:0.4f}\")\n",
    "    dbscan_clustered_local = DBSCAN(eps = eps_val, min_samples = min_samples_val).fit(dbscan_source)\n",
    "    cluster_ids_local = dbscan_clustered_local.labels_\n",
    "    if verbose:\n",
    "        print(f\"result: {np.unique(cluster_ids_local)}\")\n",
    "    try:\n",
    "        assert len(np.unique(cluster_ids_local)) <= max_n_clusters\n",
    "        dbscan_clustered = dbscan_clustered_local\n",
    "        cluster_ids = cluster_ids_local\n",
    "    except AssertionError:\n",
    "        break\n",
    "#\n",
    "print(f\"final result: {np.unique(cluster_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6596bd-70a9-4aef-8891-1f2005b780f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 必要に応じて Seaborn を道入\n",
    "#!pip install -U seaborn\n",
    "#!pip install -U adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7991f952-b482-4b17-b4d8-db8060421275",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "## 日本語表示のための設定\n",
    "plt.rcParams[\"font.family\"] = \"Hiragino sans\" # Windows/Linux の場合は別のフォントを指定\n",
    "## 凡例の文字の大きさを指定\n",
    "param_vals = {'legend.fontsize': 7, 'legend.handlelength': 2}\n",
    "plt.rcParams.update(param_vals)\n",
    "## 描画\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "d1, d2 = dbscan_source[:,0], dbscan_source[:,1]\n",
    "sns.scatterplot(x = d1, y = d2, hue = [ f\"cluster {l}\"\n",
    "    for l in dbscan_clustered.labels_ ]) # requires Searborn\n",
    "## 文字を表示する事例のサンプリング\n",
    "relative = True\n",
    "if relative:\n",
    "    lab_sampling_rate = 0.02 # サンプリング率の指定\n",
    "    lab_sample_n = round(lab_sampling_rate * len(df))\n",
    "else:\n",
    "    lab_sample_n = 30 # 絶対数の指定\n",
    "## 事例名の生成\n",
    "texts = [ ]\n",
    "label_size = max_doc_length\n",
    "sampled_keys = [ x[:label_size] for x in random.sample(list(df[doc_type]), lab_sample_n) ]\n",
    "for x, y, s in zip(d1, d2, sampled_keys):\n",
    "    texts.append(plt.text(x, y, s, size = 7, color = 'blue'))\n",
    "## label に repel を追加: adjustText package の導入が必要\n",
    "adjust_text(texts,\n",
    "        #force_points = 0.2,\n",
    "        ## Comment out the following line if you get AttributionError\n",
    "        force_text = (.1, .2),\n",
    "        #expand_points = (1, 1),\n",
    "        #expand_text = (1, 1),\n",
    "        arrowprops = dict(arrowstyle = \"-\", color = 'black', lw = 0.5))\n",
    "    \n",
    "## 題名を指定\n",
    "## used_domains\n",
    "used_domains = f\"<{', '.join(selected_domains.keys())}>\"\n",
    "## title_header\n",
    "df_size = len(df)\n",
    "if use_UMAP:\n",
    "    title_header = f\"2D view of UMAP (metric: {umap_metric}, n_neighbors: {umap_n_neighbors}; min_dist: {umap_min_dist}) for {df_size} doc encodings in\\n{used_domains} via\\n\"\n",
    "else:\n",
    "    title_header = f\"2D view of t-SNE (ppl: {doc_perplexity_val}) for {df_size} document encodings in\\n{used_domains} via\\n\"\n",
    "## title_val\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_body = f\"LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_boday = f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "else:\n",
    "    title_val = f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "## clustering_method\n",
    "clustering_method = f\"\\nclustered by DBCAN (eps: {eps_val:0.3f}; min_samples: {min_samples_val})\"\n",
    "#\n",
    "title_val = title_header + title_body + clustering_method\n",
    "plt.title(title_val)\n",
    "plt.show()\n",
    "## 局在の程度は character 1-gram, (skippy) 2-gram, (skippy) 3-gram のどれを使うかで違って来る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1d0fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## co-clusteredness に基づく領域間類似度の計算\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "if use_UMAP:\n",
    "    dbscan_source = doc_UMAP_3d\n",
    "else:\n",
    "    dbscan_source = doc_tSNE_3d\n",
    "## eps, min_samples は事例ごとに調節が必要\n",
    "min_samples_val = 2\n",
    "## co-clustering correlation の精度を上げるために，クラスター数を多目にする\n",
    "scaling_factor = 5 # 4 は必須\n",
    "max_n_clusters_for_correl = round(len(selected_domains) * scaling_factor)\n",
    "print(f\"max_n_clusters_for_correl\")\n",
    "print(f\"Looking for the optimal value for eps...\")\n",
    "check = False\n",
    "x_dbscan_clustered = None\n",
    "x_cluster_ids = None\n",
    "max_val = 10 # needs to be effectively large\n",
    "eps_vals = np.arange(max_val, 0.01, -0.01)\n",
    "eps_val_final = None\n",
    "for eps_val in eps_vals:\n",
    "    if check:\n",
    "        print(f\"testing eps = {eps_val:0.4f}\")\n",
    "    x_dbscan_clustered_local = DBSCAN(eps = eps_val, min_samples = min_samples_val).fit(dbscan_source)\n",
    "    x_cluster_ids_local = x_dbscan_clustered_local.labels_\n",
    "    if check:\n",
    "        print(f\"result: {np.unique(x_cluster_ids_local)}\")\n",
    "    try:\n",
    "        assert len(np.unique(x_cluster_ids_local)) <= max_n_clusters_for_correl\n",
    "        eps_val_final = eps_val\n",
    "        x_dbscan_clustered = x_dbscan_clustered_local\n",
    "        x_cluster_ids = x_cluster_ids_local\n",
    "    except AssertionError:\n",
    "        break\n",
    "#\n",
    "print(f\"final result: {np.unique(x_cluster_ids)}\")\n",
    "## clusterごとに言語の帰属数を集計 \n",
    "selector_var = 'domain'\n",
    "bindings = zip(encoded_df[doc_type], encoded_df[selector_var], x_dbscan_clustered.labels_)\n",
    "binding_df = pd.DataFrame(bindings, columns = [doc_type, selector_var, 'cluster'])\n",
    "selector_names = sorted(set(binding_df[selector_var]))\n",
    "print(f\"selector_names: {selector_names}\")\n",
    "cluster_ids = sorted(set(binding_df['cluster']))\n",
    "print(f\"cluster ids: {cluster_ids}\")\n",
    "clusterwise_counts = { selector_name : None for selector_name in selector_names }\n",
    "for selector_name in selector_names:\n",
    "    counts = [ ]\n",
    "    selected = binding_df[ binding_df[selector_var] == selector_name ]\n",
    "    for i, cluster_id in enumerate(cluster_ids):\n",
    "        matched = selected[ selected['cluster'] == cluster_id ]\n",
    "        n_matches = len(matched)\n",
    "        if n_matches == 0:\n",
    "            counts.append(0)\n",
    "        else:\n",
    "            counts.append(int(n_matches))\n",
    "    clusterwise_counts[selector_name] = counts\n",
    "## クラスターでの生起個数の間の相関を計算\n",
    "clusterwise_counts_df = pd.DataFrame.from_dict(clusterwise_counts) ## Truly versatile\n",
    "clusterwise_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f9c9ef",
   "metadata": {},
   "source": [
    "Heatmap で co-clusteredness 相関を可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2081a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap で可視化\n",
    "import seaborn as sns\n",
    "counts_df_normalized = (clusterwise_counts_df - clusterwise_counts_df.min())/(clusterwise_counts_df.max() - clusterwise_counts_df.min())\n",
    "corr_df = counts_df_normalized.corr()\n",
    "#corr_df.sort_index(axis = 0, inplace = True)\n",
    "#corr_df.sort_index(axis = 1, inplace = True)\n",
    "n_selectors = len(selector_names)\n",
    "fig = plt.figure(figsize = (round(n_selectors * 0.9), round(n_selectors * 0.7)))\n",
    "sns.heatmap(corr_df, cmap = sns.color_palette('coolwarm', 10),\n",
    "            annot = True, fmt = '.2f', vmin = -1, vmax = 1)\n",
    "\n",
    "title_header = f\"Heatmap of correlations on co-clusteredness over\\n{len(cluster_ids)} clusters (DBSCAN: eps: {eps_val_final:0.3f}, min_samples: {min_samples_val}) with {len(dbscan_source)} {doc_type}s from {n_selectors} {selector_var}s via\\n\"\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_body = f\"LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "    else:\n",
    "        title_body = f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\"\n",
    "else:\n",
    "    title_body = f\"LDA ({n_topics} topics; term_type: {lda_term_type})\"\n",
    "title_val = title_header + title_body\n",
    "plt.title(title_val)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f07287",
   "metadata": {},
   "source": [
    "階層クラスタリングのための再サンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120459a-dc7a-42c9-bddc-97af49e74aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 階層クラスタリングのための事例サンプリング\n",
    "hc_sampling_rate = 0.1 # 変更可能: 大きくし過ぎると図が見にくい\n",
    "df_size = len(df_filtered)\n",
    "if df_size > 600:\n",
    "    hc_df_sampled = df_filtered.sample(round(df_size * hc_sampling_rate), replace = False)\n",
    "else:\n",
    "    hc_df_sampled = df_filtered\n",
    "##\n",
    "print(f\"{len(hc_df_sampled)} rows are sampled (preserve rate: {len(hc_df_sampled)/len(df_filtered):0.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bf62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## HCデータの保存\n",
    "if save_df_sampled:\n",
    "    import datetime as dt\n",
    "    ct = dt.datetime.now()\n",
    "    ## Pandas で .csv として\n",
    "    import pandas as pd\n",
    "    # .csv \n",
    "    output_as_csv = f\"saves/df_sampled-{ct.date()}-{str(ct.time())[:2]}\" + \".csv\"\n",
    "    print(f\"saving data to {output_as_csv}\")\n",
    "    hc_df_sampled.to_csv(output_as_csv, encoding = \"utf-8\")\n",
    "    # .xlsx\n",
    "    output_as_xlsx = f\"saves/df_sampled-{ct.date()}-{str(ct.time())[:2]}\" + \".xlsx\"\n",
    "    print(f\"saving data to {output_as_xlsx}\")\n",
    "    hc_df_sampled.to_excel(output_as_xlsx)\n",
    "    ## pickle.dump(..)で\n",
    "    output_fn2 = f\"saves/df_sampled-{ct.date()}-{str(ct.time())[:2]}\" + \".p\"\n",
    "    import pickle\n",
    "    print(f\"saving data to {output_fn2}\")\n",
    "    with open(output_fn2, \"wb\") as f:\n",
    "        pickle.dump(hc_df_sampled, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f04c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## domain 事例数の確認\n",
    "hc_df_sampled['domain'].value_counts(sort = True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c56414e",
   "metadata": {},
   "source": [
    "階層クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c98443-9d46-4d22-a51a-10f4029b7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plotly が必要な場合に実行\n",
    "#!pip install -U plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10f398",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc 階層クラスタリングの実行\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "## 日本語表示のための設定\n",
    "plt.rcParams[\"font.family\"] = \"Hiragino sans\" # Windows/Linux では別のフォントを指定\n",
    "## 描画サイズの指定\n",
    "plt.figure(figsize = (6, round(10 * len(hc_df_sampled) * 0.017))) # This needs to be run here, before dendrogram construction.\n",
    "## 距離行列の構築\n",
    "linkage_methods = [ 'centroid', 'median', 'ward' ]\n",
    "linkage_method = linkage_methods[1]\n",
    "#\n",
    "doc_linkage = linkage(list(hc_df_sampled['enc']),\n",
    "                      method = linkage_method, metric = 'euclidean')\n",
    "## 事例ラベルの生成\n",
    "label_vals = [ x[:max_doc_length] for x in list(hc_df_sampled[doc_type]) ] # truncate doc keys\n",
    "## 樹状分岐図の作成\n",
    "dendrogram(doc_linkage, orientation = 'left', labels = label_vals, leaf_font_size = 8)\n",
    "\n",
    "## 題の指定\n",
    "df_size = len(hc_df_sampled)\n",
    "sampling_rate = f\"{100 * hc_sampling_rate:.2f}\"\n",
    "domain_names = ', '.join(selected_domains.keys())\n",
    "title_body = f\"Hierarchical clustering of {df_size} docs (= {sampling_rate}% sample) in\\n\\\n",
    "    <{domain_names}> domains via\\n\"\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_val = title_body + f\"{lda_term_type}-based {encoding_method} (topics {n_topics}; dims: {ft_n_dims}, term: {ft_term_type}; window size: {ft_window_size})\"\n",
    "    else:\n",
    "        title_val = title_body + f\"{encoding_method} (dims: {n_topics}, term: {ft_term_type}; window: {ft_window_size})\"\n",
    "else:\n",
    "    title_val = title_body + f\"{lda_term_type}-based {encoding_method} ({n_topics} topics)\"\n",
    "plt.title(title_val)\n",
    "\n",
    "## ラベルに domain に対応する色を付ける\n",
    "ax = plt.gca()\n",
    "for ticker in ax.get_ymajorticklabels():\n",
    "    word = ticker.get_text()\n",
    "    ## filter matched rows\n",
    "    matched_rows = hc_df_sampled.loc[hc_df_sampled[doc_type] == word]\n",
    "    if len(matched_rows) == 1:\n",
    "        row = matched_rows\n",
    "    else:\n",
    "        row = matched_rows[0] #fails to work?\n",
    "        #row = matched_rows.iloc[0] # matched_rows[0] fails to work\n",
    "    ## extract domain_id value\n",
    "    try:\n",
    "        domain_id = int(row['domain_id'])\n",
    "        ticker.set_color(colormap[domain_id]) # id の基数調整\n",
    "    except TypeError:\n",
    "        pass\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcaabad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aberrated の混ざったdoc 階層クラスタリングの実行\n",
    "aberrations = hc_df_sampled[hc_df_sampled['aberrated'] == 1][doc_type]\n",
    "print(aberrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a89dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 逸脱ありdoc 階層クラスタリングの実行\n",
    "if len(aberrations) == 0:\n",
    "    exit(0)\n",
    "##\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "## 日本語表示のための設定\n",
    "plt.rcParams[\"font.family\"] = \"Hiragino sans\" # Windows/Linux では別のフォントを指定\n",
    "## 描画サイズの指定\n",
    "plt.figure(figsize = (6, round(10 * len(hc_df_sampled) * 0.017))) # This needs to be run here, before dendrogram construction.\n",
    "## 距離行列の構築\n",
    "doc_linkage = linkage(list(hc_df_sampled['enc']), method = 'ward', metric = 'euclidean')\n",
    "## 事例ラベルの生成\n",
    "label_vals = [ x[:max_doc_length] for x in list(hc_df_sampled[doc_type]) ] # truncate doc keys\n",
    "## 樹状分岐図の作成\n",
    "dendrogram(doc_linkage, orientation = 'left', labels = label_vals, leaf_font_size = 8)\n",
    "## 題の指定\n",
    "df_size = len(hc_df_sampled)\n",
    "sampling_rate = f\"{100 * hc_sampling_rate:.2f}\"\n",
    "domain_names = ', '.join(selected_domains.keys())\n",
    "title_header = f\"Hierarchical clustering of {df_size} docs (= {sampling_rate}% sample) in\\n<{domain_names}> domains via\\n\" \n",
    "title_tail = f\"\\nincluding aberrated medical terms\"\n",
    "if use_FastText:\n",
    "    if use_LDA:\n",
    "        title_val = title_header + f\"LDA ({n_topics} topics; term: {lda_term_type}) x FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\" + title_tail\n",
    "    else:\n",
    "        title_val = title_header + f\"FastText ({ft_n_dims} dims; term: {ft_term_type}; window: {ft_window_size})\" + title_tail\n",
    "else:\n",
    "    title_val = title_header + f\"LDA ({n_topics} topics; term: {lda_term_type})\" + title_tail\n",
    "plt.title(title_val)\n",
    "## aberrated == 1 の事例を色分けする\n",
    "ax = plt.gca()\n",
    "check = False\n",
    "for ticker in ax.get_ymajorticklabels():\n",
    "    word = ticker.get_text()\n",
    "    if check:\n",
    "        print(f\"word: {word}\")\n",
    "    if word in set(aberrations):\n",
    "        if check:\n",
    "            print(f\"aberrated word:{word}\")\n",
    "        ticker.set_color('red')\n",
    "#\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30d79b",
   "metadata": {},
   "source": [
    "Terms のエンコードとクラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe02cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "## term のLDA モデルを使ったエンコード\n",
    "## get_term_topics(..) では　minimu_probability = 0 としてもprobabaly = 0 の topic IDs が\n",
    "## 得られないので，sparse encoding しか得られない\n",
    "lda_term_sparse_enc = { term : { x[0] : x[1] for x in doc_lda.get_term_topics(tid, minimum_probability = 0) } for tid, term in diction.items() }\n",
    "## check\n",
    "pp.pprint(random.sample(lda_term_sparse_enc.items(), 3))\n",
    "print(f\"Number of terms: {len(lda_term_sparse_enc)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ed63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pandas を使って sparse_enc を full enc に変換\n",
    "lda_term_enc_df = pd.DataFrame.from_dict({ k : pd.Series(v) for k, v in lda_term_sparse_enc.items()})\n",
    "## 上で生じたNaN を0に変換\n",
    "lda_term_enc_df = lda_term_enc_df.fillna(0)    \n",
    "## データを転地\n",
    "lda_term_enc_df = lda_term_enc_df.transpose()\n",
    "lda_term_enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b27445",
   "metadata": {},
   "outputs": [],
   "source": [
    "## term のtopic の総和の分布を見る\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lda_term_enc_df.sum(axis = 1), bins = 50)\n",
    "plt.title(f\"Distribution of topic value sums to terms via LDA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a1b95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtering terms\n",
    "## density で filtering\n",
    "size0 = len(lda_term_enc_df)\n",
    "lowest_density = 0.0001\n",
    "lda_term_enc_df_filtered = lda_term_enc_df[ lda_term_enc_df.sum(axis = 1) > lowest_density ]\n",
    "size1 = len(lda_term_enc_df_filtered)\n",
    "print(f\"{size1} rows remain, discarding {size0 - size1} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b7d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sampling term_enc_df for hc\n",
    "term_hc_sampling_rate = 0.1\n",
    "sampled_term_enc_df = \\\n",
    "    lda_term_enc_df_filtered.sample(round(len(lda_term_enc_df_filtered) * term_hc_sampling_rate))\n",
    "#\n",
    "sampled_term_enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## term length で filtering\n",
    "min_length = 2\n",
    "max_length = 4\n",
    "len_filter = [ len(x) >= min_length and len(x) <= max_length for x in sampled_term_enc_df.index ]\n",
    "sampled_term_enc_df = sampled_term_enc_df[len_filter]\n",
    "size2 = len(sampled_term_enc_df)\n",
    "print(f\"{size2} rows remain, discarding {size1 - size2} rows\")\n",
    "#\n",
    "sampled_term_enc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb74b57",
   "metadata": {},
   "source": [
    "Term の階層クラスタリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaddf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## term の階層クラスタリングの実行\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "## 日本語表示のための設定\n",
    "plt.rcParams[\"font.family\"] = \"Hiragino sans\" # Windows/Linux では別のフォントを指定\n",
    "\n",
    "## 描画サイズの指定\n",
    "plt.figure(figsize = (6, round(10 * len(sampled_term_enc_df) * 0.017))) # This needs to be run here, before dendrogram construction.\n",
    "\n",
    "## 距離行列の構築\n",
    "linkage_methods = [ 'centroid', 'median', 'ward' ]\n",
    "linkage_method = linkage_methods[-1]\n",
    "term_linkage = linkage(sampled_term_enc_df, method = linkage_method, metric = 'euclidean')\n",
    "\n",
    "## 事例ラベルの生成\n",
    "max_term_length = max([ len(x) for x in list(sampled_term_enc_df.index)])\n",
    "label_vals = [ x[:max_term_length] for x in list(sampled_term_enc_df.index) ] # truncate doc keys\n",
    "## 樹状分岐図の作成\n",
    "dendrogram(term_linkage, orientation = 'left', labels = label_vals, leaf_font_size = 8)\n",
    "\n",
    "## 題の指定\n",
    "term_df_size = len(sampled_term_enc_df)\n",
    "sampling_rate = f\"{100 * term_hc_sampling_rate:.2f}\"\n",
    "domain_names = ', '.join(selected_domains.keys())\n",
    "title_header = f\"Hierarchical clustering of {term_df_size} terms of lengths {min_length}-{max_length} (= {term_hc_sampling_rate}% sample) in\\n\\\n",
    "    <{domain_names}> domains via\\n\"\n",
    "title_body = f\"LDA ({n_topics} topics; term: {lda_term_type})\"\n",
    "title_val = title_header + title_body\n",
    "plt.title(title_val)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
